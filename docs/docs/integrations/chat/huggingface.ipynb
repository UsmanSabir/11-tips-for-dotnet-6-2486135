{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UsmanSabir/11-tips-for-dotnet-6-2486135/blob/main/docs/docs/integrations/chat/huggingface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMKNljy3-qmu"
      },
      "source": [
        "# ChatHuggingFace\n",
        "\n",
        "This will help you getting started with `langchain_huggingface` [chat models](/docs/concepts/chat_models). For detailed documentation of all `ChatHuggingFace` features and configurations head to the [API reference](https://python.langchain.com/api_reference/huggingface/chat_models/langchain_huggingface.chat_models.huggingface.ChatHuggingFace.html). For a list of models supported by Hugging Face check out [this page](https://huggingface.co/models).\n",
        "\n",
        "## Overview\n",
        "### Integration details\n",
        "\n",
        "### Integration details\n",
        "\n",
        "| Class | Package | Local | Serializable | JS support | Package downloads | Package latest |\n",
        "| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |\n",
        "| [ChatHuggingFace](https://python.langchain.com/api_reference/huggingface/chat_models/langchain_huggingface.chat_models.huggingface.ChatHuggingFace.html) | [langchain-huggingface](https://python.langchain.com/api_reference/huggingface/index.html) | ✅ | beta | ❌ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain_huggingface?style=flat-square&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain_huggingface?style=flat-square&label=%20) |\n",
        "\n",
        "### Model features\n",
        "| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | Native async | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |\n",
        "| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: | :---: |\n",
        "| ✅ | ✅ | ❌ | ✅ | ✅ | ✅ | ❌ | ✅ | ✅ | ❌ |\n",
        "\n",
        "## Setup\n",
        "\n",
        "To access Hugging Face models you'll need to create a Hugging Face account, get an API key, and install the `langchain-huggingface` integration package.\n",
        "\n",
        "### Credentials\n",
        "\n",
        "Generate a [Hugging Face Access Token](https://huggingface.co/docs/hub/security-tokens) and store it as an environment variable: `HUGGINGFACEHUB_API_TOKEN`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKUtQIqj-qmy",
        "outputId": "f3b6f263-3a87-491d-8d1f-c6eb2c3c9cd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your token: ··········\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"):\n",
        "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Enter your token: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iij7TLn-qm0"
      },
      "source": [
        "### Installation\n",
        "\n",
        "| Class | Package | Local | Serializable | JS support | Package downloads | Package latest |\n",
        "| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |\n",
        "| [ChatHuggingFace](https://python.langchain.com/api_reference/huggingface/chat_models/langchain_huggingface.chat_models.huggingface.ChatHuggingFace.html) | [langchain_huggingface](https://python.langchain.com/api_reference/huggingface/index.html) | ✅ | ❌ | ❌ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain_huggingface?style=flat-square&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain_huggingface?style=flat-square&label=%20) |\n",
        "\n",
        "### Model features\n",
        "| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | Native async | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |\n",
        "| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: | :---: |\n",
        "| ✅ | ✅ | ❌ | ❌ | ❌ | ❌ | ❌ | ❌ | ❌ | ❌ |\n",
        "\n",
        "## Setup\n",
        "\n",
        "To access `langchain_huggingface` models you'll need to create a/an `Hugging Face` account, get an API key, and install the `langchain_huggingface` integration package.\n",
        "\n",
        "### Credentials\n",
        "\n",
        "You'll need to have a [Hugging Face Access Token](https://huggingface.co/docs/hub/security-tokens) saved as an environment variable: `HUGGINGFACEHUB_API_TOKEN`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYnSL18g-qm0",
        "outputId": "e185ba2d-e167-4f32-ccf1-cf0a89c8b783"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HUGGINGFACEHUB_API_TOKEN··········\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\n",
        "    \"HUGGINGFACEHUB_API_TOKEN\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkdBTAl4-qm0",
        "outputId": "92caf18e-8de3-4ef9-b7a7-8340e16e30ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet  langchain-huggingface text-generation transformers google-search-results numexpr langchainhub sentencepiece jinja2 bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twtU0J_s-qm1"
      },
      "source": [
        "## Instantiation\n",
        "\n",
        "You can instantiate a `ChatHuggingFace` model in two different ways, either from a `HuggingFaceEndpoint` or from a `HuggingFacePipeline`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxU8VEwA-qm2"
      },
      "source": [
        "### `HuggingFaceEndpoint`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2zhFhA5-qm2",
        "outputId": "1ee40be1-b53f-4ad1-cbbe-f629df01ac26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /Users/isaachershenson/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        "    repetition_penalty=1.03,\n",
        ")\n",
        "\n",
        "chat_model = ChatHuggingFace(llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNo-23Pf-qm2"
      },
      "source": [
        "### `HuggingFacePipeline`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "da32ae8ec8864ccfb480044fe2eec065",
            "ee1891b7e5f64fba88ba35f444e598fb",
            "9ff1ec7f575b42adb608c15955de7888",
            "5214696698814b919f561647a684d1e4",
            "9ac334c69a2048a0a77340cca44d8c80",
            "465ad1a51d414e0daf1cd9308455be94",
            "a329c43c3d574df0afd38c7457cc639c",
            "a736a6c4023542af8c6ecc232b823d18",
            "8bdee70b843d433e8236fff83ecda022",
            "5ecb6103e0304ae188a14d598119a361",
            "174e3cb487bd453c9c70d7614254a35e",
            "28f8c233b04b45d7800e12c785a8c4bc",
            "449dfa023dc8430fbcde94544ba01c4f"
          ]
        },
        "id": "mVnzrTEp-qm2",
        "outputId": "01bc04f3-1319-44ab-9c61-0aa1971fcdf5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da32ae8ec8864ccfb480044fe2eec065",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/638 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee1891b7e5f64fba88ba35f444e598fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ff1ec7f575b42adb608c15955de7888",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5214696698814b919f561647a684d1e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00008.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ac334c69a2048a0a77340cca44d8c80",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "465ad1a51d414e0daf1cd9308455be94",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a329c43c3d574df0afd38c7457cc639c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00004-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a736a6c4023542af8c6ecc232b823d18",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00005-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8bdee70b843d433e8236fff83ecda022",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00006-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ecb6103e0304ae188a14d598119a361",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00007-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "174e3cb487bd453c9c70d7614254a35e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00008-of-00008.safetensors:   0%|          | 0.00/816M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28f8c233b04b45d7800e12c785a8c4bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "449dfa023dc8430fbcde94544ba01c4f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs=dict(\n",
        "        max_new_tokens=512,\n",
        "        do_sample=False,\n",
        "        repetition_penalty=1.03,\n",
        "    ),\n",
        ")\n",
        "\n",
        "chat_model = ChatHuggingFace(llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1RJZvOm-qm3"
      },
      "source": [
        "### Instatiating with Quantization\n",
        "\n",
        "To run a quantized version of your model, you can specify a `bitsandbytes` quantization config as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKixg6wr-qm3"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLtBfpsX-qm3"
      },
      "source": [
        "and pass it to the `HuggingFacePipeline` as a part of its `model_kwargs`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kE0eRpc-qm3"
      },
      "outputs": [],
      "source": [
        "llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs=dict(\n",
        "        max_new_tokens=512,\n",
        "        do_sample=False,\n",
        "        repetition_penalty=1.03,\n",
        "        return_full_text=False,\n",
        "    ),\n",
        "    model_kwargs={\"quantization_config\": quantization_config},\n",
        ")\n",
        "\n",
        "chat_model = ChatHuggingFace(llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mntxo5r-qm3"
      },
      "source": [
        "## Invocation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrBJP6nY-qm3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import (\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You're a helpful assistant\"),\n",
        "    HumanMessage(\n",
        "        content=\"What happens when an unstoppable force meets an immovable object?\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "ai_msg = chat_model.invoke(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zf3qLAY8-qm3",
        "outputId": "912b7acb-378c-4a07-8c66-6fb14e0a7b20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "According to the popular phrase and hypothetical scenario, when an unstoppable force meets an immovable object, a paradoxical situation arises as both forces are seemingly contradictory. On one hand, an unstoppable force is an entity that cannot be stopped or prevented from moving forward, while on the other hand, an immovable object is something that cannot be moved or displaced from its position. \n",
            "\n",
            "In this scenario, it is un\n"
          ]
        }
      ],
      "source": [
        "print(ai_msg.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lXOiqqg-qm3"
      },
      "source": [
        "## API reference\n",
        "\n",
        "For detailed documentation of all `ChatHuggingFace` features and configurations head to the API reference: https://python.langchain.com/api_reference/huggingface/chat_models/langchain_huggingface.chat_models.huggingface.ChatHuggingFace.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0_980xq-qm3"
      },
      "source": [
        "## API reference\n",
        "\n",
        "For detailed documentation of all ChatHuggingFace features and configurations head to the API reference: https://python.langchain.com/api_reference/huggingface/chat_models/langchain_huggingface.chat_models.huggingface.ChatHuggingFace.html"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}